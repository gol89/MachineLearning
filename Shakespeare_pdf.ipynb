{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = keras.utils.get_file('shakespeare.txt',shakespeare_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='C:\\\\Users\\\\Besitzer\\\\.keras\\\\datasets\\\\shakespeare.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.count>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1a78cf7aeb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26, 6, 33, 5, 16]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Vijay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 5 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55769"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function from_tensor_slices in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "from_tensor_slices(tensors)\n",
      "    Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "    \n",
      "    The given tensors are sliced along their first dimension. This operation\n",
      "    preserves the structure of the input tensors, removing the first dimension\n",
      "    of each tensor and using it as the dataset dimension. All input tensors\n",
      "    must have the same size in their first dimensions.\n",
      "    \n",
      "    >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
      "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [1, 2, 3]\n",
      "    \n",
      "    >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
      "    >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      "    \n",
      "    >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
      "    >>> # scalar tensors.\n",
      "    >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [(1, 3, 5), (2, 4, 6)]\n",
      "    \n",
      "    >>> # Dictionary structure is also preserved.\n",
      "    >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
      "    >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
      "    ...                                       {'a': 2, 'b': 4}]\n",
      "    True\n",
      "    \n",
      "    >>> # Two tensors can be combined into one Dataset object.\n",
      "    >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
      "    >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
      "    >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
      "    >>> # Both the features and the labels tensors can be converted\n",
      "    >>> # to a Dataset object separately and combined after.\n",
      "    >>> features_dataset = Dataset.from_tensor_slices(features)\n",
      "    >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
      "    >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
      "    >>> # A batched feature and label set can be converted to a Dataset\n",
      "    >>> # in similar fashion.\n",
      "    >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
      "    ...                                 [[2, 1], [1, 2]],\n",
      "    ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
      "    >>> batched_labels = tf.constant([['A', 'A'],\n",
      "    ...                               ['B', 'B'],\n",
      "    ...                               ['A', 'B']], shape=(3, 2, 1))\n",
      "    >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
      "    >>> for element in dataset.as_numpy_iterator():\n",
      "    ...   print(element)\n",
      "    (array([[1, 3],\n",
      "           [2, 3]], dtype=int32), array([[b'A'],\n",
      "           [b'A']], dtype=object))\n",
      "    (array([[2, 1],\n",
      "           [1, 2]], dtype=int32), array([[b'B'],\n",
      "           [b'B']], dtype=object))\n",
      "    (array([[3, 3],\n",
      "           [3, 2]], dtype=int32), array([[b'A'],\n",
      "           [b'B']], dtype=object))\n",
      "    \n",
      "    Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      "    enabled, the values will be embedded in the graph as one or more\n",
      "    `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      "    memory and run into byte limits of graph serialization. If `tensors`\n",
      "    contains one or more large NumPy arrays, consider the alternative described\n",
      "    in [this guide](\n",
      "    https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      "    \n",
      "    Args:\n",
      "      tensors: A dataset element, with each component having the same size in\n",
      "        the first dimension.\n",
      "    \n",
      "    Returns:\n",
      "      Dataset: A `Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data.Dataset.from_tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "       19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "        9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = n_steps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(window_length,drop_remainder=True,shift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window : window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda windows : (windows[:,:-1],windows[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences=True,input_shape=[None,max_id],dropout=0.2,recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740/1740 [==============================] - 393s 226ms/step - loss: 1.9561\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1740/1740 [==============================] - 413s 237ms/step - loss: 1.5582\n",
      "Epoch 2/10\n",
      "1740/1740 [==============================] - 415s 238ms/step - loss: 1.4564\n",
      "Epoch 3/10\n",
      "1740/1740 [==============================] - 401s 230ms/step - loss: 1.4029\n",
      "Epoch 4/10\n",
      "1740/1740 [==============================] - 403s 232ms/step - loss: 1.3661\n",
      "Epoch 5/10\n",
      "1740/1740 [==============================] - 415s 239ms/step - loss: 1.3393\n",
      "Epoch 6/10\n",
      "1740/1740 [==============================] - 418s 240ms/step - loss: 1.3188\n",
      "Epoch 7/10\n",
      "1740/1740 [==============================] - 422s 243ms/step - loss: 1.3011\n",
      "Epoch 8/10\n",
      "1740/1740 [==============================] - 419s 241ms/step - loss: 1.2868\n",
      "Epoch 9/10\n",
      "1740/1740 [==============================] - 482s 277ms/step - loss: 1.2735\n",
      "Epoch 10/10\n",
      "1740/1740 [==============================] - 697s 401ms/step - loss: 1.2625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1740/1740 [==============================] - 686s 395ms/step - loss: 1.2530\n",
      "Epoch 2/3\n",
      "1740/1740 [==============================] - 705s 405ms/step - loss: 1.2439\n",
      "Epoch 3/3\n",
      "1740/1740 [==============================] - 688s 395ms/step - loss: 1.2360\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1 \n",
    "    return tf.one_hot(X,max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess(['How are yo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 39), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-51-2aa0796cd133>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 13,  0,  6,  8,  1,  0,  7,  3, 13]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets us generate some fake Shakespearean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text,temperature):\n",
    "    X_new = preprocess([text])\n",
    "    y_prob = model.predict(X_new)[0,-1:,:]\n",
    "    rescaled_logits = tf.math.log(y_prob)/temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits,num_samples=1)+1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text,n_chars=500,temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text,temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the people,\n",
      "when they have made them the common party,\n",
      "when we have been such a perpetual consul.\n",
      "\n",
      "menenius:\n",
      "the marketplace, some brown, some blows\n",
      "most did not to be stand for the blood who have been so many grave been seven them to be\n",
      "monstrous marcius coriolanus come off,\n",
      "and the blood upon the marketplace,\n",
      "our world grave for the people,\n",
      "our proud to be say him stand upon the common party,\n",
      "which he should be bless to him as the common party,\n",
      "when he had rather have not to be swords to do,\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"a\",temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pine, and i think if he did\n",
      "re-quicken'd them as the people,\n",
      "our proud to be say him stand for the people,\n",
      "our noble call'd af the people,\n",
      "but the man i' the common many-headed and their best to do; for the world\n",
      "be swords to the common many grave not to be\n",
      "many grave not to see him stand for the common battle call'd\n",
      "to see him stand for the people,\n",
      "our world: i do owe the world: i thank the people,\n",
      "when he will be blood to be some bloody for the people,\n",
      "but the man i' the people,\n",
      "who have see hi\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"p\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellown the people,\n",
      "our proud to be ingrateful power to the common made all the lives\n",
      "and the man i' the people,\n",
      "our proud to be some blows he had rather have he did\n",
      "report them the common made all joy and to be\n",
      "many stand for the blood of the people,\n",
      "our proved with shunn coriolanus and to be some bloody for the people.\n",
      "\n",
      "second citizen:\n",
      "we are so many grave men the common many-headed and the common for the people,\n",
      "when he had rather have the people,\n",
      "when he had rather have the people,\n",
      "i will be blow\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"hello\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeare1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellos,\n",
      "as your world: doing the world: marcius coriolanus\n",
      "much mamaius i' the sounds and loves taken to the city,\n",
      "let caius marcius brutus.\n",
      "\n",
      "menenius:\n",
      "she do speak to make a human praised was one cannot\n",
      "be cannot to have him for hearing them. he hath we have\n",
      "fast true! iw his bread and i shall\n",
      "be nottemn cafath'd the people, let's field.\n",
      "\n",
      "sicinius:\n",
      "the fliers!\n",
      "o the noble deeds; never thought\n",
      "with the nicer, she have donation'd\n",
      "way o'er the linds one this stay it's intening\n",
      "with speak, some out of h\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"hello\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pent in all the blind. how is he cames the sun\n",
      "with thind unoort: o that the miles.\n",
      "\n",
      "menenius:\n",
      "i had rather shall be sarr in blood\n",
      "upon 't: but my word: and fair, he cannot act\n",
      "that for noon, he stopp'd against the bleade had rather\n",
      "gracious so our\n",
      "the city, i fought the common for and report him good\n",
      "mution.\n",
      "\n",
      "brutus:\n",
      "havill'd inclined in what come.\n",
      "\n",
      "coriolanus:\n",
      "you give us.\n",
      "\n",
      "menenius:\n",
      "i sin the people,\n",
      "your fliers; neither welcome to the least call'd me, i that which\n",
      "i will be the field when i t\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"p\", temperature=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
